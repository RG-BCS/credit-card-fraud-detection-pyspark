{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Credit Card Fraud Detection with PySpark\n",
        "\n",
        "Goal: Build a logistic regression model (with and without class weighting) to detect fraudulent credit card transactions using PySpark's MLlib.\n",
        "\n",
        "Why PySpark? Spark scales well with large datasets, supports distributed computing, and integrates well with ML workflows using its MLlib library."
      ],
      "metadata": {
        "id": "ft5iEBNsuUj6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Setup and Imports"
      ],
      "metadata": {
        "id": "arjLOES7uh5M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n2JY0NqECBY1"
      },
      "outputs": [],
      "source": [
        "# PySpark Setup\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, when\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "# Evaluation with sklearn\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Create Spark Session"
      ],
      "metadata": {
        "id": "2ZbmS7w-ut0L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder\\\n",
        "        .appName(\"Credit Card Fraud Detection\")\\\n",
        "        .getOrCreate()\n"
      ],
      "metadata": {
        "id": "nJ8LIsOeuw5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Load and Inspect the Dataset"
      ],
      "metadata": {
        "id": "zgaMJSSkuy2Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load CSV dataset\n",
        "df = spark.read.csv(\"/kaggle/input/creditcardfraud/creditcard.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Show schema and sample data\n",
        "df.printSchema()\n",
        "df.show(5)\n"
      ],
      "metadata": {
        "id": "L2QP67ypu3dS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Preprocessing and Train-Test Split"
      ],
      "metadata": {
        "id": "4tj3F1pCu6do"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.withColumn(\"label\", col(\"Class\").cast(\"integer\")).drop(\"Class\")\n",
        "df = df.na.drop()\n",
        "\n",
        "train_data, test_data = df.randomSplit([0.8, 0.2], seed=42)\n"
      ],
      "metadata": {
        "id": "py0SPmd0vA1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Address Class Imbalance with Weighting"
      ],
      "metadata": {
        "id": "z83Lx4CsvFbT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assign higher weight to minority class (fraud)\n",
        "class_weight = 20.0\n",
        "train_weighted = train_data.withColumn(\"weight\", when(col(\"label\") == 1, class_weight).otherwise(1.0))\n",
        "test_weighted = test_data.withColumn(\"weight\", when(col(\"label\") == 1, class_weight).otherwise(1.0))\n"
      ],
      "metadata": {
        "id": "f2pgR0NHvJLh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Feature Engineering with VectorAssembler"
      ],
      "metadata": {
        "id": "pBfyG0q3vOMq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select features\n",
        "feature_cols = [col for col in df.columns if col not in (\"label\", \"weight\")]\n",
        "\n",
        "# Assemble feature vector\n",
        "assembler = VectorAssembler(inputCols=feature_cols, outputCol='features')\n",
        "train_data_assembled = assembler.transform(train_data).select(\"features\", \"label\")\n",
        "test_data_assembled = assembler.transform(test_data).select(\"features\", \"label\")\n"
      ],
      "metadata": {
        "id": "d_-vplGMvSTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Train Logistic Regression Models\n",
        "\n",
        ". Unweighted logistic regression\n",
        "\n",
        ". Weighted logistic regression using a Spark Pipeline"
      ],
      "metadata": {
        "id": "iTDOoJkDvVBx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
        "lr_weighted = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", weightCol=\"weight\")\n",
        "\n",
        "# Train models\n",
        "lr_model = lr.fit(train_data_assembled)\n",
        "\n",
        "pipeline = Pipeline(stages=[assembler, lr_weighted])\n",
        "lr_weighted_model = pipeline.fit(train_weighted)\n"
      ],
      "metadata": {
        "id": "Hk_hIoIXvYrA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Evaluate Model Performance"
      ],
      "metadata": {
        "id": "EYBKMD6nvpPo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate with AUC\n",
        "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"label\")\n",
        "\n",
        "predictions = lr_model.transform(test_data_assembled)\n",
        "predictions_weighted = lr_weighted_model.transform(test_weighted)\n",
        "\n",
        "auc = evaluator.evaluate(predictions)\n",
        "auc_weighted = evaluator.evaluate(predictions_weighted)\n",
        "\n",
        "print(f\"AUC (Unweighted): {auc:.4f}\")\n",
        "print(f\"AUC (Weighted):   {auc_weighted:.4f}\")\n"
      ],
      "metadata": {
        "id": "RZSx_53-vq04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. Classification Report and Confusion Matrix"
      ],
      "metadata": {
        "id": "Y-JwGOuTvx_d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert predictions to pandas\n",
        "y_pred = predictions.select(\"prediction\").toPandas()\n",
        "y_true = predictions.select(\"label\").toPandas()\n",
        "\n",
        "y_pred_w = predictions_weighted.select(\"prediction\").toPandas()\n",
        "y_true_w = predictions_weighted.select(\"label\").toPandas()\n",
        "\n",
        "# Print classification reports\n",
        "print(\"\\nUnweighted Model Report:\\n\", classification_report(y_true, y_pred))\n",
        "print(\"\\nWeighted Model Report:\\n\", classification_report(y_true_w, y_pred_w))\n"
      ],
      "metadata": {
        "id": "tJ9SGkM4vv0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10. Confusion Matrix Visualization"
      ],
      "metadata": {
        "id": "qdt-FKnev46B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot confusion matrices\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "cm_w = confusion_matrix(y_true_w, y_pred_w)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "ConfusionMatrixDisplay(confusion_matrix=cm).plot(ax=axes[0])\n",
        "axes[0].set_title(\"Unweighted Model\")\n",
        "\n",
        "ConfusionMatrixDisplay(confusion_matrix=cm_w).plot(ax=axes[1])\n",
        "axes[1].set_title(\"Weighted Model\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "2RNhiGX9v6FT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary\n",
        "\n",
        "- PySpark was used to process a real-world, imbalanced dataset\n",
        "\n",
        "- Logistic Regression was trained using both unweighted and class-weighted approaches\n",
        "\n",
        "- Evaluation included AUC, classification reports, and confusion matrices\n",
        "\n",
        "- The weighted model significantly improved detection of fraudulent cases"
      ],
      "metadata": {
        "id": "hZOjYqPwv821"
      }
    }
  ]
}